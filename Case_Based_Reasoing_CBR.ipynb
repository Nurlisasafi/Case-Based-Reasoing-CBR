{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Do04P4lXsxUA"
   },
   "source": [
    "# Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5mZ4k-Z4DsOB"
   },
   "source": [
    "extract pdf with pdfminer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o7svoTP7Ucje",
    "outputId": "d5288852-3842-4d84-9544-2a4fe40be20b"
   },
   "outputs": [],
   "source": [
    "pip install pdfminer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "25IFVQXOUjEP",
    "outputId": "e396f79e-040c-4af1-b26f-e3e21ff4aa8d"
   },
   "outputs": [],
   "source": [
    "!pip install pdfminer.six"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "12vWUQ_6UvmZ",
    "outputId": "0bd0805b-49e1-4228-89dc-b065f7ec0b11"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Buat folder jika belum ada\n",
    "os.makedirs(\"data/pdf\", exist_ok=True)\n",
    "\n",
    "# Pindahkan file ke folder data/pdf\n",
    "for filename in uploaded.keys():\n",
    "    shutil.move(filename, f\"data/pdf/{filename}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YFAs4cvBD2tm"
   },
   "source": [
    "Convert & Extraction Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bS-b86KDVFq7"
   },
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "import os\n",
    "import re\n",
    "\n",
    "# Lokasi folder\n",
    "input_folder = 'data/pdf/'\n",
    "output_folder = 'data/convert/'  # jangan pakai '/' di depan kalau bukan root\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Ambil semua file PDF\n",
    "pdf_files = [f for f in os.listdir(input_folder) if f.endswith(\".pdf\")]\n",
    "\n",
    "for idx, filename in enumerate(pdf_files, 1):\n",
    "    pdf_path = os.path.join(input_folder, filename)\n",
    "    text = extract_text(pdf_path)\n",
    "\n",
    "    # 1. Bersihkan dan gabungkan setiap baris dengan spasi\n",
    "    lines = [line.strip() for line in text.splitlines() if line.strip()]\n",
    "    combined = ' '.join(lines)\n",
    "\n",
    "    # 2. Sisipkan spasi di antara huruf kecil dan huruf kapital yang menempel (contoh: \"JiminTempat\")\n",
    "    combined = re.sub(r'([a-z])([A-Z])', r'\\1 \\2', combined)\n",
    "\n",
    "    # 3. Hilangkan spasi ganda akibat proses join\n",
    "    cleaned_text = re.sub(r'\\s{2,}', ' ', combined)\n",
    "\n",
    "    # Simpan hasil ekstraksi ke file .txt dengan nama putusan001.txt, dst.\n",
    "    output_path = os.path.join(output_folder, f\"putusan{idx:03}.txt\")\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(cleaned_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KqjThQDrD85t"
   },
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nlWd0fo8VdpH"
   },
   "outputs": [],
   "source": [
    "def clean_header_footer(text):\n",
    "    lines = text.splitlines()  # Pisah jadi list per baris\n",
    "    cleaned_lines = []\n",
    "\n",
    "    for line in lines:\n",
    "        # Hilangkan kata-kata target dari tiap baris\n",
    "        line = line.replace(\"Direktori Putusan Mahkamah Agung Republik Indonesia\", \"\")\n",
    "        line = line.replace(\"Mahkamah Agung Republik Indonesia\", \"\")\n",
    "        line = line.replace(\"putusan.mahkamahagung.go.id\", \"\")\n",
    "\n",
    "        # Hapus baris jika kosong setelah dibersihkan\n",
    "        if line.strip():  # cek jika masih ada isi\n",
    "            cleaned_lines.append(line.strip())\n",
    "\n",
    "    return '\\n'.join(cleaned_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2pfj6c24WEd9"
   },
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "def clean_footer(text):\n",
    "    cleaned_text = re.sub(\n",
    "        r\"Disclaimer[\\s\\S]*?(Email\\s*:\\s*.+?[\\n\\r]+)?Telp\\s*:\\s*\\d{3}[-\\s]?\\d{3,4}(?:[-\\s]?\\d{3,4})?\\s*\",\n",
    "        \"\",\n",
    "        text,\n",
    "        flags=re.IGNORECASE\n",
    "    )\n",
    "    return cleaned_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JJcnXw9wdwLO"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def normalize_text(text):\n",
    "    text = text.lower()                             # lowercase semua\n",
    "    text = re.sub(r'\\b(?:[a-zA-Z]\\s+){2,}[a-zA-Z]\\b', lambda m: m.group(0).replace(' ', ''), text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur4nGYj0Wcku"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_putusan_text(text):\n",
    "    cleaned_lines = []\n",
    "\n",
    "    # Pola watermark halaman\n",
    "    pattern = re.compile(\n",
    "        r'^\\s*(Hal\\.?|Halaman)\\s+\\d+\\s+(dari|/)\\s+\\d+\\s+Putusan\\s+Nomor\\s+[0-9./\\-]+/[a-zA-Z]+/[0-9]{4}/PN\\s+\\w+\\s*$',\n",
    "        flags=re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    for line in text.splitlines():\n",
    "        # Hanya hapus baris yang benar-benar cocok watermark\n",
    "        if not pattern.match(line.strip()):\n",
    "            cleaned_lines.append(line)\n",
    "\n",
    "    return \"\\n\".join(cleaned_lines).strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dM6_NkOAVwdL"
   },
   "outputs": [],
   "source": [
    "def clean_text_pipeline(text):\n",
    "    text = clean_header_footer(text)\n",
    "    text = clean_footer(text)\n",
    "    text = clean_putusan_text(text)\n",
    "\n",
    "    # Hapus semua \"Halaman X\"\n",
    "    text = re.sub(r\"(?i)\\bhalaman\\s*\\d+(\\s*dari\\s*\\d+)?\\b\", \"\", text)\n",
    "\n",
    "    text = normalize_text(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-jf2mkS4V3uk"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "input_folder = \"data/convert\"\n",
    "output_folder = \"data/raw\"\n",
    "\n",
    "# Pastikan folder output ada\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Ambil semua file .txt dari folder input dan urutkan\n",
    "txt_files = sorted([f for f in os.listdir(input_folder) if f.endswith(\".txt\")])\n",
    "\n",
    "for i, filename in enumerate(txt_files, start=1):\n",
    "    input_path = os.path.join(input_folder, filename)\n",
    "\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Bersihkan teks dengan pipeline\n",
    "    cleaned_text = clean_text_pipeline(text)\n",
    "\n",
    "    # Format nama file output: case_001.txt, case_002.txt, ...\n",
    "    output_filename = f\"case_{i:03}.txt\"\n",
    "    output_path = os.path.join(output_folder, output_filename)\n",
    "\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c33JmB5NEVc6"
   },
   "source": [
    "# representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NK67S0E2WZgz"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def nomor_perkara(text):\n",
    "    # Tangkap pola nomor perkara umum seperti \"167/Pid.Sus/2024/PN Mlg\"\n",
    "    match = re.search(\n",
    "        r'\\d+\\s*/\\s*Pid\\.Sus\\s*/\\s*\\d{4}\\s*/\\s*PN\\s+\\w+',\n",
    "        text,\n",
    "        flags=re.IGNORECASE\n",
    "    )\n",
    "    if match:\n",
    "        return match.group()\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J05uzeS91A3G"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def amar_putusan(text):\n",
    "    # Normalisasi format penomoran\n",
    "    text = re.sub(r\"(\\d+)\\.(\\S)\", r\"\\1. \\2\", text)\n",
    "    text = re.sub(r\";\\s*(\\d+)\\.\", r\"; \\1.\", text)\n",
    "\n",
    "    # Deteksi 'mengadili'\n",
    "    pattern_mengadili = re.compile(r\"(m\\s*e\\s*n\\s*g\\s*a\\s*d\\s*i\\s*l\\s*i\\s*:?)|\\bmengadili\\s*:?\", flags=re.IGNORECASE)\n",
    "    matches = list(pattern_mengadili.finditer(text))\n",
    "    if not matches:\n",
    "        return \"Tidak ditemukan kata 'mengadili'\"\n",
    "\n",
    "    # Ambil teks setelah 'mengadili'\n",
    "    last_match = matches[-1]\n",
    "    amar_text = text[last_match.end():].strip()\n",
    "\n",
    "    # Gabungkan baris terpisah\n",
    "    amar_text = re.sub(r'\\n+', ' ', amar_text)\n",
    "\n",
    "    # Ambil poin-poin bernomor\n",
    "    poin_matches = re.findall(r\"\\d+\\.\\s+(.*?)(?=(?:\\d+\\.\\s)|$)\", amar_text, flags=re.DOTALL)\n",
    "    cleaned = []\n",
    "\n",
    "    if poin_matches:\n",
    "        for poin in poin_matches[:5]:\n",
    "            sentence = re.sub(r'\\s+', ' ', poin.strip())\n",
    "\n",
    "            # Deteksi dan ganti frasa denda yang tidak lengkap dengan terbilang\n",
    "            if re.search(r\"denda\\s+sejumlah\\s+Rp[^\\w]*\", sentence, flags=re.IGNORECASE):\n",
    "                # Cari frasa terbilang di kalimat berikutnya (asumsinya langsung lanjut)\n",
    "                terbilang_match = re.search(r\"\\((satu .*?rupiah)\\)\", text, flags=re.IGNORECASE)\n",
    "                if terbilang_match:\n",
    "                    terbilang = terbilang_match.group(1).strip()\n",
    "                    sentence = re.sub(r\"denda\\s+sejumlah\\s+Rp[^\\s]*\", f\"denda sejumlah {terbilang}\", sentence, flags=re.IGNORECASE)\n",
    "                else:\n",
    "                    # fallback jika tidak ditemukan terbilang\n",
    "                    sentence = re.sub(r\"denda\\s+sejumlah\\s+Rp[^\\s]*\", \"denda sejumlah tidak diketahui\", sentence, flags=re.IGNORECASE)\n",
    "\n",
    "            if not sentence.endswith('.'):\n",
    "                sentence += '.'\n",
    "            cleaned.append(sentence)\n",
    "    else:\n",
    "        # fallback kalimat biasa\n",
    "        kalimat_matches = re.split(r'(?<=[.;])\\s+', amar_text)\n",
    "        for k in kalimat_matches[:2]:\n",
    "            sentence = re.sub(r'\\s+', ' ', k.strip())\n",
    "            if not sentence.endswith('.'):\n",
    "                sentence += '.'\n",
    "            cleaned.append(sentence)\n",
    "\n",
    "    # Ambil dua kalimat paling relevan\n",
    "    pidana_denda = [s for s in cleaned if 'pidana' in s.lower() or 'denda' in s.lower()]\n",
    "    result = pidana_denda[:2] if len(pidana_denda) >= 2 else cleaned[:2]\n",
    "\n",
    "    return ' '.join(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LLW8NScgfQp8"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_pekerjaan(text):\n",
    "    # 1. Bagi berdasarkan terdakwa: pakai romawi diikuti angka dan titik (misalnya: i.1., ii.1., dst)\n",
    "    # Asumsikan bagian terdakwa diakhiri sebelum \"mengadili\" atau \"menimbang\" atau teks utama dimulai\n",
    "    parts = re.split(r\"(?=\\b(?:i{1,3}|iv|v|vi|vii|viii|ix|x)[.]\\s*1[.])\", text, flags=re.IGNORECASE)\n",
    "\n",
    "    pekerjaan_list = []\n",
    "\n",
    "    for part in parts:\n",
    "        # 2. Cari 'pekerjaan' di setiap bagian\n",
    "        match = re.search(\n",
    "            r\"(?i)pekerjaan\\s*:?\\s*(.+?)(?=\\s*(?:agama|pendidikan|terdakwa|status|[0-9]{1,2}[.)])\\b|[;\\n\\.]|$)\",\n",
    "            part\n",
    "        )\n",
    "        if match:\n",
    "            pekerjaan = re.sub(r'\\s+', ' ', re.sub(r',+', '', match.group(1))).strip()\n",
    "            pekerjaan_list.append(pekerjaan)\n",
    "\n",
    "    return pekerjaan_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gPVwbXHdfUrr"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_penuntut_umum(text):\n",
    "    # Tangani jika HANYA ada Penuntut Umum TANPA nama\n",
    "    pattern_tanpa_nama = r\"(?:dengan|serta)[\\s]*dihadiri[\\s]*oleh[\\s]*(?:Jaksa[\\s]*Penuntut[\\s]*Umum|Penuntut[\\s]*Umum|JaksaPenuntutUmum|PenuntutUmum)(?![\\w])\"\n",
    "    if re.search(pattern_tanpa_nama, text, re.IGNORECASE | re.DOTALL):\n",
    "        return \"tidak disebutkan\"\n",
    "\n",
    "    # Tangani jika nama disebutkan sebelum frasa penuntut umum\n",
    "    pattern_nama = r\"(?:dengan|serta)[\\s]*dihadiri[\\s]*oleh[\\s]*([\\w\\s.,()-]+?)\\s*(?:selaku|sebagai)?\\s*(?:Jaksa[\\s]*Penuntut[\\s]*Umum|Penuntut[\\s]*Umum|JaksaPenuntutUmum|PenuntutUmum)\"\n",
    "    match = re.search(pattern_nama, text, re.IGNORECASE | re.DOTALL)\n",
    "    if match:\n",
    "        return match.group(1).strip()\n",
    "\n",
    "    return \"tidak ada informasi\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uWJXCK0PfYX7"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def nama_terdakwa(text):\n",
    "    pattern = r\"(?i)(?:\\d\\.\\s*)?(?:nama lengkap|nama)\\s*:?\\s*([\\s\\S]{3,100}?)(?=\\s*(?:\\d?[.)]?\\s*)?tempat\\s+lahir)\"\n",
    "    return [m.strip() for m in re.findall(pattern, text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n9o65pB1fe8N"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_ringkasan_barang_bukti(text):\n",
    "    # Ambil bagian setelah \"mengadili\"\n",
    "    mengadili_matches = list(re.finditer(r\"mengadili\", text, flags=re.IGNORECASE))\n",
    "    if not mengadili_matches:\n",
    "        return \"tidak ditemukan bagian mengadili\"\n",
    "\n",
    "    last_mengadili = mengadili_matches[-1].end()\n",
    "    sisa_text = text[last_mengadili:]\n",
    "\n",
    "    # Temukan frasa \"barang bukti berupa\", fleksibel terhadap spasi & newline\n",
    "    match_bb = re.search(\n",
    "        r\"(?:menetapkan|menyatakan)?(?:\\s+|\\n+)*(?:agar\\s+)?barang(?:\\s+|\\n+)bukti(?:\\s+|\\n+)?(?:masing[-\\s]masing\\s+)?berupa(?:\\s+|\\n+)*:?\",\n",
    "        sisa_text,\n",
    "        flags=re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    if not match_bb:\n",
    "        return \"tidak ditemukan bagian barang bukti berupa\"\n",
    "\n",
    "    start_idx = match_bb.end()\n",
    "    after_bb = sisa_text[start_idx:]\n",
    "\n",
    "    # Potong sampai sebelum bagian penutup\n",
    "    stop_match = re.search(r\"(membebankan|putusan|demikianlah|pasal|hakim)\", after_bb, flags=re.IGNORECASE)\n",
    "    if stop_match:\n",
    "        after_bb = after_bb[:stop_match.start()]\n",
    "\n",
    "    # Ambil poin-poin barang bukti\n",
    "    poin_match = re.findall(r\"[-•\\d]+[.)]?\\s*(.+?)(?=(?:\\n[-•\\d]+[.)]?\\s)|\\Z)\", after_bb.strip(), flags=re.DOTALL)\n",
    "\n",
    "    if not poin_match:\n",
    "        return \"tidak ditemukan poin barang bukti\"\n",
    "\n",
    "    empat_barang = [re.sub(r\"\\s+\", \" \", item.strip()) for item in poin_match[:4]]\n",
    "    return \"Barang bukti berupa: \" + \"; \".join(f\"- {item}\" for item in empat_barang) + \".\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s7RcrNlhfjN5"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_metadata(text):\n",
    "    data = {}\n",
    "\n",
    "    # Nomor perkara\n",
    "    data['nomor_perkara'] = nomor_perkara(text)\n",
    "\n",
    "    # m = re.findall(r\"nama lengkap\\s*:?\\s*([a-zA-Z.,\\-\\s]+)\", text, flags=re.IGNORECASE)\n",
    "    data['nama_terdakwa'] = nama_terdakwa(text)\n",
    "\n",
    "    # Pekerjaan\n",
    "    data['pekerjaan'] = extract_pekerjaan(text)\n",
    "\n",
    "    # perkara\n",
    "    m = re.search(r\"\\b(tindak pidana.*?)\\b(narkotika|psikotropika|prekursor narkotika)\\b\", text, flags=re.IGNORECASE)\n",
    "    data['tindak pidana'] = m.group(2).capitalize() if m else \"tidak ada informasi\"\n",
    "\n",
    "    # Pasal\n",
    "    m = re.search(r\"Pasal\\s+[^\\n]*?Tahun\\s+\\d{4}\", text, flags=re.IGNORECASE)\n",
    "    data['pasal'] = m.group(0).strip() if m else \"\"\n",
    "\n",
    "    # penuntut umum / penggugat\n",
    "    data['penuntut_umum'] = extract_penuntut_umum(text)\n",
    "\n",
    "    # Tanggal (optional)\n",
    "    m = re.search(r\"(diputuskan|putusan)[^\\n]*tanggal\\s+(\\d{1,2}\\s+[A-Za-z]+\\s+\\d{4})\", text, flags=re.IGNORECASE)\n",
    "    data['tanggal_putusan'] = m.group(2).strip() if m else \"\"\n",
    "\n",
    "    # penuntut umum / penggugat\n",
    "    data['amar_putusan'] = amar_putusan(text)\n",
    "\n",
    "    # penuntut umum / penggugat\n",
    "    data['barang_bukti'] = extract_ringkasan_barang_bukti(text)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 222
    },
    "id": "6l_MroFIfnQX",
    "outputId": "208338ec-3a10-4145-d4ad-fc326b674aed"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Misal nama file yang mau diuji\n",
    "filename = \"data/raw/case_013.txt\"\n",
    "\n",
    "# Baca file\n",
    "with open(os.path.join(filename), 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "    metadata = extract_metadata(text)\n",
    "    metadata['case_id'] = \"case_001\"\n",
    "    metadata['text_full'] = text\n",
    "\n",
    "# Simpan ke DataFrame (1 baris saja)\n",
    "df = pd.DataFrame([metadata])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rVsXWmDAqBoZ",
    "outputId": "b377b6c9-e4cc-43ed-b972-4036d950a60c"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Folder input\n",
    "input_dir = \"data/raw\"\n",
    "\n",
    "# List untuk menampung metadata semua file\n",
    "all_data = []\n",
    "\n",
    "# Loop semua file .txt\n",
    "for filename in os.listdir(input_dir):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(input_dir, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            text = f.read()\n",
    "            metadata = extract_metadata(text)\n",
    "            metadata['case_id'] = os.path.splitext(filename)[0]\n",
    "            metadata['text_full'] = text\n",
    "            all_data.append(metadata)\n",
    "\n",
    "# Buat DataFrame dari semua metadata\n",
    "df = pd.DataFrame(all_data)\n",
    "# Hitung length (jumlah kata)\n",
    "df[\"length_text_full\"] = df[\"text_full\"].astype(str).apply(lambda x: len(x.split()))\n",
    "\n",
    "# Tampilkan DataFrame\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "27laY6aKs6_0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Buat folder jika belum ada\n",
    "os.makedirs(\"data/processed\", exist_ok=True)\n",
    "\n",
    "# Simpan file\n",
    "df.to_json(\"data/processed/cases.json\", orient='records', indent=2, force_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GnF9afl4EjgV"
   },
   "source": [
    "# retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 660
    },
    "id": "koEnn9EbtdSN",
    "outputId": "5df90d37-9c0f-4cdd-9636-f0453c6394c7"
   },
   "outputs": [],
   "source": [
    "df_loaded = pd.read_json(\"data/processed/cases.json\")\n",
    "df_loaded.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XokT7SqutrVB",
    "outputId": "45849751-677a-4743-98b8-ed4a96f2846d"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n4mwGclsu3Rx"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 301,
     "referenced_widgets": [
      "6b1ee6023bfb4c50b5ca3823f3dc1a20",
      "14037a412637438a8d732d77627053a0",
      "58afbc2e5c55422e863db2ed49e4bcd8",
      "48fae76d940b458ea507ae580c7abd34",
      "4b7d38819a5642d584d5ded07f2718f2",
      "5210bea9f8c54dd7bf91c7eb06d2f43e",
      "f13302f25a84403ab58f09a478925010",
      "56707bd4021844c2b78ac74554950213",
      "f5a861de9b534335b66d23ec02249c61",
      "2c7157fffaad40ef8a84fcec413d0383",
      "9b1368a2e60048f5947d891eacd3bb53",
      "0693e17b4899444d9d9b92818ca1edad",
      "ff7bbe0f15964eceaf93c5050794e1c4",
      "ace6f90674ab43108b3f821a01ac05b9",
      "55cca629049a444db1b562edfc15729e",
      "6a6d43eef2e14ecc9b741f2649e6441b",
      "6d47a3c9efdd4262ae79f297eb02a600",
      "ac3e590315fe4b9e9a609ad0e75b3a87",
      "f2cfb36a5073468aa721c0897dd69c3d",
      "7300dac27cdf4ad1bc612028b5f1cea0",
      "cd2308c137914e7aabd84468751ae121",
      "325f879173824e42a6526aa85a808856",
      "504c1032d2af4916aeb3bad763945920",
      "afbbdcca93c04feb9b2c35a0abf95517",
      "a1f783cb363440d8aee7308ccf81ee34",
      "e354733dfe914f1bbfee04684aedadd9",
      "5eadab12edbf43a98491387696a9dcbb",
      "8199ee7f7ed044c38665c917297e0774",
      "3e933a727e4549f4920eb5b872e035c6",
      "cf83e38e66c54afb94b39c0c0f4d63d1",
      "5573db20269e4991a05b84460244f84d",
      "78740542f3bc460b8d988bc63ed9ac1b",
      "1319c79bcc85470997a4346d719db435",
      "443371517cc3429da4bbebcc7819c9ad",
      "c0f0298f33c944c39ebcc42b95084992",
      "130a3e499a044d2dbda71645771f252b",
      "3ec94dbcc6ac49259e802974e5d1f118",
      "b97f3f199da74e819aa3ac6402ef525f",
      "25584c3c030949908670ee4059a06c6b",
      "1ea10982f6624b058cc7cefd68d920fe",
      "7ef5d9b2a5234f76a81a5cdc180523a6",
      "9a0676e13b4a407495d3e216fa4cda9b",
      "90d0561047ac4d5ea72cb6a4cda07d51",
      "449d2290e38045609897a2248cfc6401",
      "dcae58d7de954212975abdfe8cdb6f2b",
      "879075336fc1479cb9d23fba8a0b7971",
      "bf657fc2e0cc4d639a6ad4457187d775",
      "65ebb6c02b044d929fe1c2821bb604bd",
      "c9c287ce4be94fe9ad14f4684a698aa1",
      "d9348646add44b3195d38b8539a12e6e",
      "23320ce9bd034486a87509a0ba07ef80",
      "0628f3efb8d5430295bc7686b24f0f84",
      "d0a254a409834a17b65a6c8b7e32b23c",
      "ba1945fd423d46e4be905a41bace9fc6",
      "5a1e171b41b34bc7abb28c3bbc193641"
     ]
    },
    "id": "oTzXiAjTu-h5",
    "outputId": "bb1d88fb-4752-4ce9-f2b1-9ea42401f033"
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"indobenchmark/indobert-base-p1\")\n",
    "model = AutoModel.from_pretrained(\"indobenchmark/indobert-base-p1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6R-QZqKEvJOO"
   },
   "outputs": [],
   "source": [
    "def get_bert_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    cls_embedding = outputs.last_hidden_state[:, 0, :]\n",
    "    return cls_embedding.squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "ad4088c3802346d6925653bb5059c1de",
      "566c9899a8c74b6bb26e2cc422b134ce",
      "dd03cb79abb94ff88cc92906e38fedf5",
      "c44546c224744769ad2c67d1de9620c0",
      "f59bf48608534bed843e2a025807e466",
      "c88a7c3cf92942f080f814afc9eb9db5",
      "e7ecc159bf1e44eab454150eb0013abc",
      "a60161b3170c48ed80d9340cfc7b8c69",
      "935c576e5ff544f4a67713ed60c4b8fa",
      "938f015856a64bce9e9d0cad69f1512a",
      "6b9820ca8eed4ffab7c5488a1abc877c"
     ]
    },
    "id": "ELPH92dWvrDt",
    "outputId": "bf1c7c6f-70c4-4640-d61a-17d59d5e09c3"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load file data terstruktur dari tahap Case Representation\n",
    "df = pd.read_json(\"data/processed/cases.json\")\n",
    "\n",
    "# Ambil barang_bukti + pasal\n",
    "case_texts = [\n",
    "    (str(row[\"barang_bukti\"]) + \" \" + str(row[\"pasal\"]))\n",
    "    for _, row in df.iterrows()\n",
    "]\n",
    "case_ids = df[\"case_id\"].tolist()\n",
    "\n",
    "# Hitung embedding semua kasus\n",
    "case_embeddings = [get_bert_embedding(text) for text in case_texts]\n",
    "\n",
    "# Simpan embedding dan case_ids\n",
    "np.save(\"data/processed/bert_embeddings.npy\", case_embeddings)\n",
    "with open(\"data/processed/case_ids.json\", \"w\") as f:\n",
    "    json.dump(case_ids, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jTT_BwuEwNuU"
   },
   "outputs": [],
   "source": [
    "texts_train, texts_test, ids_train, ids_test = train_test_split(\n",
    "    case_texts, case_ids, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Simpan test queries\n",
    "queries = [{\"query_id\": i, \"text\": texts_test[i], \"true_case_id\": ids_test[i]} for i in range(len(texts_test))]\n",
    "os.makedirs(\"data/eval\", exist_ok=True)\n",
    "with open(\"data/eval/queries.json\", \"w\") as f:\n",
    "    json.dump(queries, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bpqMB69wwUK0"
   },
   "outputs": [],
   "source": [
    "def retrieve(query, k=5):\n",
    "    query_vec = get_bert_embedding(query).reshape(1, -1)\n",
    "\n",
    "    # Load case base vectors & ids\n",
    "    case_vecs = np.load(\"data/processed/bert_embeddings.npy\")\n",
    "    with open(\"data/processed/case_ids.json\") as f:\n",
    "        case_ids = json.load(f)\n",
    "\n",
    "    similarities = cosine_similarity(query_vec, case_vecs)[0]\n",
    "    top_k_indices = similarities.argsort()[-k:][::-1]\n",
    "    return [case_ids[i] for i in top_k_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "02FMrIG2wYcX",
    "outputId": "16394f1b-eb87-4b1c-aee1-08baffab3e13"
   },
   "outputs": [],
   "source": [
    "# Tes ambil satu query dari queries.json\n",
    "with open(\"data/eval/queries.json\") as f:\n",
    "    test_queries = json.load(f)\n",
    "\n",
    "for q in test_queries[:3]:  # ambil 3 contoh awal\n",
    "    print(f\"\\nQuery: {q['text'][:100]}...\")\n",
    "    top_cases = retrieve(q['text'], k=5)\n",
    "    print(f\"Top-5 retrieved: {top_cases}\")\n",
    "    print(f\"Ground truth: {q['true_case_id']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y8JSZo3HFaOO"
   },
   "source": [
    "# predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9gHBRo7cwlz-"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "df = pd.read_json(\"data/processed/cases.json\")\n",
    "\n",
    "# Buat mapping case_id → amar_putusan\n",
    "case_solutions = {\n",
    "    row[\"case_id\"]: row[\"amar_putusan\"]\n",
    "    for _, row in df.iterrows()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3iykvOrWxMJe"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def predict_outcome(query: str, k: int = 5) -> str:\n",
    "    top_k = retrieve(query, k=k)\n",
    "    solutions = [case_solutions.get(c, \"\") for c in top_k]\n",
    "\n",
    "    # Majority vote\n",
    "    most_common = Counter(solutions).most_common(1)\n",
    "    predicted_solution = most_common[0][0] if most_common else \"\"\n",
    "\n",
    "    return predicted_solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1zO_Nl9TxPqf"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Load queries\n",
    "with open(\"data/eval/queries.json\") as f:\n",
    "    test_queries = json.load(f)\n",
    "\n",
    "# Jalankan prediksi\n",
    "results = []\n",
    "for q in test_queries:\n",
    "    pred_sol = predict_outcome(q['text'])\n",
    "    top_k = retrieve(q['text'], k=5)\n",
    "\n",
    "    results.append({\n",
    "        \"query_id\": q['query_id'],\n",
    "        \"predicted_solution\": pred_sol,\n",
    "        \"top_5_case_ids\": top_k\n",
    "    })\n",
    "\n",
    "# Simpan ke CSV\n",
    "df_preds = pd.DataFrame(results)\n",
    "\n",
    "# Buat folder 'data/results' jika belum ada\n",
    "os.makedirs(\"data/results\", exist_ok=True)\n",
    "df_preds.to_csv(\"data/results/predictions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y8Fh8vyjxj31",
    "outputId": "d85cc2b0-62cb-438c-abc3-fb67baa0ee97"
   },
   "outputs": [],
   "source": [
    "# perbandingan prediksi dan putusan sebenarnya\n",
    "for q in test_queries[:5]:\n",
    "    query = q[\"text\"]\n",
    "    predicted = predict_outcome(query)\n",
    "    true_case_id = q[\"true_case_id\"]\n",
    "    true_solution = case_solutions[true_case_id]\n",
    "\n",
    "    print(f\"\\nQuery ID       : {q['query_id']}\")\n",
    "    print(f\"Predicted      : {predicted}\")\n",
    "    print(f\"Ground Truth   : {true_solution}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kHEkPKNCFjk4"
   },
   "source": [
    "# evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kvqkeGhrCj_C"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def eval_retrieval(queries, k=5):\n",
    "    data = []\n",
    "\n",
    "    for q in queries:\n",
    "        top_k = retrieve(q[\"text\"], k=k)\n",
    "        true_id = q[\"true_case_id\"]\n",
    "\n",
    "        # Binary label: 1 jika benar ada di top-k, 0 jika tidak\n",
    "        y_true = [1]\n",
    "        y_pred = [1 if true_id in top_k else 0]\n",
    "\n",
    "        data.append({\n",
    "            \"query_id\": q[\"query_id\"],\n",
    "            \"y_true\": y_true[0],\n",
    "            \"y_pred\": y_pred[0]\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Hitung metrik\n",
    "    acc = accuracy_score(df[\"y_true\"], df[\"y_pred\"])\n",
    "    prec = precision_score(df[\"y_true\"], df[\"y_pred\"], zero_division=0)\n",
    "    rec = recall_score(df[\"y_true\"], df[\"y_pred\"], zero_division=0)\n",
    "    f1 = f1_score(df[\"y_true\"], df[\"y_pred\"], zero_division=0)\n",
    "\n",
    "    # Simpan ke CSV\n",
    "    os.makedirs(\"data/eval\", exist_ok=True)\n",
    "    df.to_csv(\"data/eval/retrieval_metrics.csv\", index=False)\n",
    "\n",
    "    print(\"== Retrieval Evaluation ==\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall: {rec:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7gGImZ0-C0I-"
   },
   "outputs": [],
   "source": [
    "def eval_prediction(queries, case_solutions):\n",
    "    data = []\n",
    "\n",
    "    for q in queries:\n",
    "        true_sol = case_solutions[q[\"true_case_id\"]]\n",
    "        predicted_sol = predict_outcome(q[\"text\"])\n",
    "\n",
    "        label_true = true_sol.strip().lower()\n",
    "        label_pred = predicted_sol.strip().lower()\n",
    "\n",
    "        data.append({\n",
    "            \"query_id\": q[\"query_id\"],\n",
    "            \"y_true\": label_true,\n",
    "            \"y_pred\": label_pred\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Konversi label jadi kategori unik\n",
    "    labels = list(set(df[\"y_true\"]) | set(df[\"y_pred\"]))\n",
    "    acc = accuracy_score(df[\"y_true\"], df[\"y_pred\"])\n",
    "    prec = precision_score(df[\"y_true\"], df[\"y_pred\"], average='macro', zero_division=0)\n",
    "    rec = recall_score(df[\"y_true\"], df[\"y_pred\"], average='macro', zero_division=0)\n",
    "    f1 = f1_score(df[\"y_true\"], df[\"y_pred\"], average='macro', zero_division=0)\n",
    "\n",
    "    # Simpan ke CSV\n",
    "    df.to_csv(\"data/eval/prediction_metrics.csv\", index=False)\n",
    "\n",
    "    print(\"\\n== Prediction Evaluation ==\")\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Precision: {prec:.4f}\")\n",
    "    print(f\"Recall: {rec:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pn0rKDfTC6WJ",
    "outputId": "5ad4aebb-e8de-47e3-8b7b-bbc2c97f70cb"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "with open(\"data/eval/queries.json\") as f:\n",
    "    test_queries = json.load(f)\n",
    "\n",
    "# Panggil evaluasi\n",
    "eval_retrieval(test_queries, k=5)\n",
    "eval_prediction(test_queries, case_solutions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "slPzPGi0DBZ1",
    "outputId": "b0024fbd-2b2e-4a5e-d5c2-976cdc2c2e60"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load hasil retrieval\n",
    "df_retrieval = pd.read_csv(\"data/eval/retrieval_metrics.csv\")\n",
    "\n",
    "# Hitung metrik agregat\n",
    "acc = accuracy_score(df_retrieval[\"y_true\"], df_retrieval[\"y_pred\"])\n",
    "prec = precision_score(df_retrieval[\"y_true\"], df_retrieval[\"y_pred\"], zero_division=0)\n",
    "rec = recall_score(df_retrieval[\"y_true\"], df_retrieval[\"y_pred\"], zero_division=0)\n",
    "f1 = f1_score(df_retrieval[\"y_true\"], df_retrieval[\"y_pred\"], zero_division=0)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar([\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"], [acc, prec, rec, f1], color=\"skyblue\")\n",
    "plt.ylim(0, 1)\n",
    "plt.title(\"Retrieval Performance Metrics\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"data/eval/retrieval_metrics_chart.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "id": "dRTfK2XRDJEi",
    "outputId": "827c2d11-4389-4f76-e343-e8c5b0dfe256"
   },
   "outputs": [],
   "source": [
    "# Load hasil prediction\n",
    "df_pred = pd.read_csv(\"data/eval/prediction_metrics.csv\")\n",
    "\n",
    "# Hitung metrik agregat\n",
    "acc_pred = accuracy_score(df_pred[\"y_true\"], df_pred[\"y_pred\"])\n",
    "prec_pred = precision_score(df_pred[\"y_true\"], df_pred[\"y_pred\"], average='macro', zero_division=0)\n",
    "rec_pred = recall_score(df_pred[\"y_true\"], df_pred[\"y_pred\"], average='macro', zero_division=0)\n",
    "f1_pred = f1_score(df_pred[\"y_true\"], df_pred[\"y_pred\"], average='macro', zero_division=0)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar([\"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\"], [acc_pred, prec_pred, rec_pred, f1_pred], color=\"lightcoral\")\n",
    "plt.ylim(0, 1)\n",
    "plt.title(\"Prediction Performance Metrics\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"data/eval/prediction_metrics_chart.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gBb1oksmDQ5C"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Baca ulang hasil prediksi\n",
    "df_pred = pd.read_csv(\"data/eval/prediction_metrics.csv\")\n",
    "\n",
    "# Tampilkan prediksi yang salah\n",
    "for i, row in df_pred.iterrows():\n",
    "    if row[\"y_true\"] != row[\"y_pred\"]:\n",
    "        print(f\"Query ID: {row['query_id']}\")\n",
    "        print(f\"True    : {row['y_true']}\")\n",
    "        print(f\"Pred    : {row['y_pred']}\\n\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
